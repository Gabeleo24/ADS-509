{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95952cac",
   "metadata": {},
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n",
    "\n",
    "This notebook has two parts. In the first part, you will scrape lyrics from AZLyrics.com. In the second part, you'll run code that verifies the completeness of your data pull. \n",
    "\n",
    "For this assignment you have chosen two musical artists who have at least 20 songs with lyrics on AZLyrics.com. We start with pulling some information and analyzing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580daa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a29797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting song links for each artist...\n",
      "Processing cory_asbury...\n",
      "Found 0 songs for cory_asbury\n",
      "Processing hillsong_united...\n",
      "Found 0 songs for hillsong_united\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Verify we have enough songs for each artist\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m artist, lp \u001b[38;5;129;01min\u001b[39;00m lyrics_pages.items():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lp)) > \u001b[32m20\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martist\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m we have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m songs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEstimated time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lp)*\u001b[32m10\u001b[39m/\u001b[32m60\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Define the artists to scrape\n",
    "artists = {\n",
    "    'cory_asbury': \"https://www.azlyrics.com/c/coryasbury.html\",\n",
    "    'hillsong_united': \"https://www.azlyrics.com/h/hillsongunited.html\"\n",
    "}\n",
    "\n",
    "# Set up a dictionary to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "# Scrape the artist pages to get links to individual song lyrics\n",
    "print(\"Collecting song links for each artist...\")\n",
    "for artist, artist_page in artists.items():\n",
    "    print(f\"Processing {artist}...\")\n",
    "    \n",
    "    # Request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 5*random.random())  # Be nice to the server\n",
    "    \n",
    "    # Extract the links to lyrics pages\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    album_list = soup.find('div', {'class': 'album-list'})\n",
    "    \n",
    "    if album_list:\n",
    "        # Get all links to song lyrics\n",
    "        links = album_list.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            if '/lyrics/' in link['href']:\n",
    "                # Add the full URL to our dictionary\n",
    "                lyrics_pages[artist].append('https://www.azlyrics.com' + link['href'])\n",
    "    \n",
    "    print(f\"Found {len(lyrics_pages[artist])} songs for {artist}\")\n",
    "\n",
    "# Verify we have enough songs for each artist\n",
    "for artist, lp in lyrics_pages.items():\n",
    "    assert(len(set(lp)) > 20)\n",
    "    print(f\"For {artist} we have {len(lp)} songs.\")\n",
    "    print(f\"Estimated time: {round(len(lp)*10/60, 2)} minutes\")\n",
    "    \n",
    "# Function to generate filenames from links\n",
    "def generate_filename_from_link(link):\n",
    "    if not link:\n",
    "        return None\n",
    "    \n",
    "    # Drop the http or https and the html\n",
    "    name = link.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    name = name.replace(\".html\", \"\")\n",
    "    name = name.replace(\"www.azlyrics.com/lyrics/\", \"\")\n",
    "    \n",
    "    # Replace special characters with underscore\n",
    "    name = name.replace(\".\", \"_\").replace(\"/\", \"_\")\n",
    "    \n",
    "    # Add .txt extension\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Create the lyrics folder\n",
    "print(\"\\nCreating lyrics folder...\")\n",
    "if os.path.isdir(\"lyrics\"):\n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "os.mkdir(\"lyrics\")\n",
    "\n",
    "# Scrape the lyrics\n",
    "print(\"\\nScraping lyrics...\")\n",
    "start = time.time()\n",
    "total_pages = 0\n",
    "\n",
    "for artist in lyrics_pages:\n",
    "    # Create a subfolder for the artist\n",
    "    artist_dir = os.path.join(\"lyrics\", artist)\n",
    "    os.mkdir(artist_dir)\n",
    "    \n",
    "    print(f\"\\nStarting to scrape lyrics for {artist}...\")\n",
    "    \n",
    "    # Limit to 25 songs per artist to save time\n",
    "    for page_url in lyrics_pages[artist][:25]:\n",
    "        try:\n",
    "            # Request the lyrics page\n",
    "            response = requests.get(page_url)\n",
    "            time.sleep(5 + 5*random.random())  # Be nice to the server\n",
    "            \n",
    "            # Extract the title and lyrics\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the title\n",
    "            title_div = soup.find('div', {'class': 'ringtone'})\n",
    "            title = \"Unknown Title\"\n",
    "            if title_div and title_div.find_previous('b'):\n",
    "                title = title_div.find_previous('b').text.strip()\n",
    "            \n",
    "            # Find the lyrics\n",
    "            lyrics_div = soup.find('div', {'class': None, 'id': None}, \n",
    "                                  attrs={'style': None})\n",
    "            if lyrics_div:\n",
    "                lyrics = lyrics_div.text.strip()\n",
    "                \n",
    "                # Write out the title and lyrics\n",
    "                filename = generate_filename_from_link(page_url)\n",
    "                with open(os.path.join(artist_dir, filename), 'w', encoding='utf-8') as f:\n",
    "                    f.write(title + '\\n\\n' + lyrics)\n",
    "                \n",
    "                total_pages += 1\n",
    "                print(f\"Saved lyrics for {artist} - {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {page_url}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal run time: {round((time.time() - start)/60, 2)} minutes\")\n",
    "print(f\"Total pages scraped: {total_pages}\")\n",
    "\n",
    "# Function to extract words from text\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# Analyze the lyrics\n",
    "print(\"\\nAnalyzing lyrics...\")\n",
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "# Create a dictionary to store word counts for each artist\n",
    "artist_word_counts = {}\n",
    "\n",
    "for artist in artist_folders:\n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"\\nFor {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "    for f_name in artist_files:\n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name, encoding='utf-8') as infile:\n",
    "            artist_words.extend(words(infile.read()))\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n",
    "    \n",
    "    # Store word counts for this artist\n",
    "    artist_word_counts[artist] = Counter(artist_words)\n",
    "    \n",
    "    # Print the most common words\n",
    "    print(f\"Most common words for {artist}:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"{'Word':<15} | {'Count':<10}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for word, count in [(w, c) for w, c in artist_word_counts[artist].most_common(50) if len(w) > 2][:10]:\n",
    "        print(f\"{word:<15} | {count:<10}\")\n",
    "\n",
    "# Try to create visualizations if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    # Plot the most common words for each artist\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (artist, word_counts) in enumerate(artist_word_counts.items()):\n",
    "        # Get the 10 most common words with length > 2\n",
    "        common_words = [(word, count) for word, count in word_counts.most_common(50) if len(word) > 2][:10]\n",
    "        words_list, counts = zip(*common_words)\n",
    "        \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.barh(words_list, counts)\n",
    "        plt.title(f\"Most Common Words for {artist}\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(\"artist_word_comparison.png\")\n",
    "    plt.show()\n",
    "    print(\"Visualization saved as 'artist_word_comparison.png'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not create visualizations: {e}\")\n",
    "    print(\"Continuing with text-based analysis only.\")\n",
    "\n",
    "# Compare word usage between artists\n",
    "print(\"\\nComparing word usage between artists:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find common words across all artists\n",
    "all_words = set()\n",
    "for word_counts in artist_word_counts.values():\n",
    "    all_words.update(word for word, _ in word_counts.most_common(100) if len(word) > 2)\n",
    "\n",
    "# Select some interesting words to compare\n",
    "comparison_words = list(all_words)[:20]  # Just use the first 20 words\n",
    "\n",
    "# Print a comparison table\n",
    "print(f\"{'Word':<15}\", end=\"\")\n",
    "for artist in artist_folders:\n",
    "    print(f\"| {artist:<15}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (15 + 17 * len(artist_folders)))\n",
    "\n",
    "for word in comparison_words:\n",
    "    print(f\"{word:<15}\", end=\"\")\n",
    "    for artist in artist_folders:\n",
    "        count = artist_word_counts[artist].get(word, 0)\n",
    "        print(f\"| {count:<15}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c891db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
